{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos for training:  1735\n",
      "Number of videos for testing:  215\n"
     ]
    }
   ],
   "source": [
    "# Includes, global variables, initial preprocessing (Dataset folder parsing)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, CuDNNLSTM, ZeroPadding3D, TimeDistributed\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import ntpath\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Folder to the UFC Dataset\n",
    "UFC_FOLDER = \"/home/box/Downloads/UCF-Anomaly-Detection-Dataset/\"\n",
    "# Last # videos from each folder to be used for testing\n",
    "NUM_FILES_FOR_TESTING = 5\n",
    "# Categories for videos\n",
    "CATEGORIES = [\"Normal\", \"Anomaly\"]\n",
    "# Resolution to use (ex: 100x100)\n",
    "FRAME_REZ = 100\n",
    "# Number of frames in one batch\n",
    "NUM_FRAMES_IN_BATCH = 100\n",
    "# Skip factor (ex: if set to 4, then remove first quater of frames from batch,\n",
    "#   shift everything to the begining and populate created space with new frames)\n",
    "SKIP_FACTOR = 1.25 # 4\n",
    "\n",
    "def getListOfAnomFolders():\n",
    "    folders = []\n",
    "    for folder in os.listdir(UFC_FOLDER):\n",
    "        if \"Normal\" not in folder:\n",
    "            folders.append(folder)\n",
    "    return folders\n",
    "\n",
    "def getTrainingVids(anomFolders):\n",
    "    train_vids = []\n",
    "\n",
    "    # Add all tuples of (normal categorie, normal videos paths) to the training list\n",
    "    normal_vids = os.path.join(UFC_FOLDER, \"Normal_Videos_event\")\n",
    "    for vid in os.listdir(normal_vids):\n",
    "        if \"mp4\" in vid:\n",
    "            train_vids.append((CATEGORIES.index(\"Normal\"), os.path.join(normal_vids, vid)))\n",
    "            pass\n",
    "\n",
    "    # Add all tuples of (normal categorie, normal videos paths) to the training list\n",
    "    normal_vids = os.path.join(UFC_FOLDER, \"Training_Normal_Videos_Anomaly\")\n",
    "    for vid in os.listdir(normal_vids):\n",
    "        if \"mp4\" in vid:\n",
    "            train_vids.append((CATEGORIES.index(\"Normal\"), os.path.join(normal_vids, vid)))\n",
    "\n",
    "    # Add all tuples of (anomalous categorie, anomalous video path) to the reaining list\n",
    "    for anomFolder in anomFolders:\n",
    "        tmp_fol = os.path.join(UFC_FOLDER, anomFolder)\n",
    "        for vid in os.listdir(tmp_fol)[:-NUM_FILES_FOR_TESTING]:\n",
    "            if \"mp4\" in vid:\n",
    "                train_vids.append((CATEGORIES.index(\"Anomaly\"), os.path.join(tmp_fol, vid)))\n",
    "                \n",
    "    # Randomise list for better training\n",
    "    random.shuffle(train_vids)\n",
    "    return train_vids\n",
    "\n",
    "def getTestingVids(anomFolders):\n",
    "    test_vids = []\n",
    "\n",
    "    # Add all tuples of (normal categorie, normal videos paths) to the testing list\n",
    "    normal_vids = os.path.join(UFC_FOLDER, \"Testing_Normal_Videos_Anomaly\")\n",
    "    for vid in os.listdir(normal_vids):\n",
    "        if \"mp4\" in vid:\n",
    "            test_vids.append((CATEGORIES.index(\"Normal\"), os.path.join(normal_vids, vid)))\n",
    "\n",
    "    # Add all tuples of (anomalous categorie, anomalous video path) to the reaining list\n",
    "    for anomFolder in anomFolders:\n",
    "        tmp_fol = os.path.join(UFC_FOLDER, anomFolder)\n",
    "        for vid in os.listdir(tmp_fol)[-NUM_FILES_FOR_TESTING:]:\n",
    "            if \"mp4\" in vid:\n",
    "                test_vids.append((CATEGORIES.index(\"Anomaly\"), os.path.join(tmp_fol, vid)))\n",
    "\n",
    "    # Randomise list for better training\n",
    "    random.shuffle(test_vids)\n",
    "    return test_vids\n",
    "\n",
    "UFC_FOLDER = os.path.join(UFC_FOLDER, \"UCF_Crimes/Videos\")\n",
    "anomFolders = getListOfAnomFolders()\n",
    "training_vid_paths = getTrainingVids(anomFolders)\n",
    "testing_vid_paths = getTestingVids(anomFolders)\n",
    "print(\"Number of videos for training: \", len(training_vid_paths))\n",
    "print(\"Number of videos for testing: \", len(testing_vid_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model No.1 input 25x100x100x\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "video = Input(shape=(NUM_FRAMES_IN_BATCH//4,\n",
    "                     FRAME_REZ, #in the example: channels,\n",
    "                     FRAME_REZ, #rows,\n",
    "                     3))        #in the example: columns))\n",
    "cnn_base = VGG16(input_shape=(FRAME_REZ, #channels,\n",
    "                              FRAME_REZ, #rows,\n",
    "                              3),        #columns),\n",
    "                 weights=\"imagenet\",\n",
    "                 include_top=False)\n",
    "cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n",
    "cnn = Model(cnn_base.input, cnn_out)\n",
    "cnn.trainable = False\n",
    "encoded_frames = TimeDistributed(cnn)(video)\n",
    "encoded_sequence = CuDNNLSTM(256)(encoded_frames)\n",
    "hidden_layer = Dense(1024, activation=\"relu\")(encoded_sequence)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "model = Model([video], outputs)\n",
    "\n",
    "optimizer = Nadam(lr=0.002,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on a Normal video (1/1735)\n",
      "Num frames:  8000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.1298 - acc: 0.8889 - val_loss: 1.0592e-04 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 698ms/step - loss: 2.5551e-05 - acc: 1.0000 - val_loss: 1.7788e-06 - val_acc: 1.0000\n",
      "data_X shape:  (4, 100, 100, 100, 3)\n",
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 2s 761ms/step - loss: 1.4450e-06 - acc: 1.0000 - val_loss: 6.8226e-07 - val_acc: 1.0000\n",
      "Working on a Anomaly video (2/1735)\n",
      "Num frames:  11100\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 696ms/step - loss: 2.2276 - acc: 0.6667 - val_loss: 0.0209 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.7176e-04 - val_acc: 1.0000\n",
      "data_X shape:  (4, 100, 100, 100, 3)\n",
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 2s 741ms/step - loss: 3.2325e-04 - acc: 1.0000 - val_loss: 2.4340e-04 - val_acc: 1.0000\n",
      "Working on a Anomaly video (3/1735)\n",
      "Num frames:  3000\n",
      "data_X shape:  (8, 100, 100, 100, 3)\n",
      "Train on 7 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 5s 705ms/step - loss: 1.7717e-04 - acc: 1.0000 - val_loss: 1.1636e-04 - val_acc: 1.0000\n",
      "Working on a Normal video (4/1735)\n",
      "Num frames:  500\n",
      "data_X shape:  (1, 100, 100, 100, 3)\n",
      "Train on 0 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "Something went wrong:  'ProgbarLogger' object has no attribute 'log_values'\n",
      "Working on a Anomaly video (5/1735)\n",
      "Num frames:  4700\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 8.7464e-05 - acc: 1.0000 - val_loss: 6.3541e-05 - val_acc: 1.0000\n",
      "data_X shape:  (4, 100, 100, 100, 3)\n",
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 2s 742ms/step - loss: 6.0441e-05 - acc: 1.0000 - val_loss: 5.4838e-05 - val_acc: 1.0000\n",
      "Working on a Anomaly video (6/1735)\n",
      "Num frames:  1800\n",
      "data_X shape:  (5, 100, 100, 100, 3)\n",
      "Train on 4 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 5.1410e-05 - acc: 1.0000 - val_loss: 4.6373e-05 - val_acc: 1.0000\n",
      "Working on a Anomaly video (7/1735)\n",
      "Num frames:  2100\n",
      "data_X shape:  (6, 100, 100, 100, 3)\n",
      "Train on 5 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 4s 715ms/step - loss: 4.2892e-05 - acc: 1.0000 - val_loss: 3.7313e-05 - val_acc: 1.0000\n",
      "Working on a Normal video (8/1735)\n",
      "Num frames:  1800\n",
      "data_X shape:  (5, 100, 100, 100, 3)\n",
      "Train on 4 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 5.8194 - acc: 0.0000e+00 - val_loss: 1.7816 - val_acc: 0.0000e+00\n",
      "Working on a Normal video (9/1735)\n",
      "Num frames:  80900\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 1.0516 - acc: 0.0000e+00 - val_loss: 0.7513 - val_acc: 0.0000e+00\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 0.7040 - acc: 0.4444 - val_loss: 0.6695 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.6602 - acc: 1.0000 - val_loss: 0.6507 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 0.6454 - acc: 1.0000 - val_loss: 0.6394 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 703ms/step - loss: 0.6347 - acc: 1.0000 - val_loss: 0.6289 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 703ms/step - loss: 0.6244 - acc: 1.0000 - val_loss: 0.6188 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 0.6145 - acc: 1.0000 - val_loss: 0.6092 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 693ms/step - loss: 0.6050 - acc: 1.0000 - val_loss: 0.5998 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.5957 - acc: 1.0000 - val_loss: 0.5907 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.5868 - acc: 1.0000 - val_loss: 0.5819 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 705ms/step - loss: 0.5781 - acc: 1.0000 - val_loss: 0.5733 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.5696 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 0.5613 - acc: 1.0000 - val_loss: 0.5568 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 0.5533 - acc: 1.0000 - val_loss: 0.5488 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.5454 - acc: 1.0000 - val_loss: 0.5411 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.5377 - acc: 1.0000 - val_loss: 0.5335 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 0.5301 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.5228 - acc: 1.0000 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.5155 - acc: 1.0000 - val_loss: 0.5116 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 703ms/step - loss: 0.5084 - acc: 1.0000 - val_loss: 0.5046 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 0.5015 - acc: 1.0000 - val_loss: 0.4977 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 0.4947 - acc: 1.0000 - val_loss: 0.4910 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.4880 - acc: 1.0000 - val_loss: 0.4844 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 0.4815 - acc: 1.0000 - val_loss: 0.4779 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 703ms/step - loss: 0.4751 - acc: 1.0000 - val_loss: 0.4715 - val_acc: 1.0000\n",
      "data_X shape:  (2, 100, 100, 100, 3)\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 919ms/step - loss: 0.4715 - acc: 1.0000 - val_loss: 0.4708 - val_acc: 1.0000\n",
      "Working on a Normal video (10/1735)\n",
      "Num frames:  900\n",
      "data_X shape:  (2, 100, 100, 100, 3)\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 899ms/step - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4701 - val_acc: 1.0000\n",
      "Working on a Normal video (11/1735)\n",
      "Num frames:  13400\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.4674 - acc: 1.0000 - val_loss: 0.4639 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 0.4612 - acc: 1.0000 - val_loss: 0.4578 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.4552 - acc: 1.0000 - val_loss: 0.4518 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 706ms/step - loss: 0.4492 - acc: 1.0000 - val_loss: 0.4459 - val_acc: 1.0000\n",
      "data_X shape:  (1, 100, 100, 100, 3)\n",
      "Train on 0 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "Something went wrong:  'ProgbarLogger' object has no attribute 'log_values'\n",
      "Working on a Anomaly video (12/1735)\n",
      "Num frames:  700\n",
      "data_X shape:  (1, 100, 100, 100, 3)\n",
      "Train on 0 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "Something went wrong:  'ProgbarLogger' object has no attribute 'log_values'\n",
      "Working on a Normal video (13/1735)\n",
      "Num frames:  1200\n",
      "data_X shape:  (3, 100, 100, 100, 3)\n",
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 2s 799ms/step - loss: 0.4456 - acc: 1.0000 - val_loss: 0.4446 - val_acc: 1.0000\n",
      "Working on a Anomaly video (14/1735)\n",
      "Num frames:  600\n",
      "data_X shape:  (1, 100, 100, 100, 3)\n",
      "Train on 0 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "Something went wrong:  'ProgbarLogger' object has no attribute 'log_values'\n",
      "Working on a Normal video (15/1735)\n",
      "Num frames:  17400\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 0.4421 - acc: 1.0000 - val_loss: 0.4389 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.4364 - acc: 1.0000 - val_loss: 0.4332 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.4307 - acc: 1.0000 - val_loss: 0.4277 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.4252 - acc: 1.0000 - val_loss: 0.4222 - val_acc: 1.0000\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 702ms/step - loss: 0.4198 - acc: 1.0000 - val_loss: 0.4168 - val_acc: 1.0000\n",
      "data_X shape:  (3, 100, 100, 100, 3)\n",
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 2s 798ms/step - loss: 0.4165 - acc: 1.0000 - val_loss: 0.4156 - val_acc: 1.0000\n",
      "Working on a Anomaly video (16/1735)\n",
      "Num frames:  2000\n",
      "data_X shape:  (5, 100, 100, 100, 3)\n",
      "Train on 4 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 1.0773 - acc: 0.0000e+00 - val_loss: 1.0747 - val_acc: 0.0000e+00\n",
      "Working on a Normal video (17/1735)\n",
      "Num frames:  1300\n",
      "data_X shape:  (3, 100, 100, 100, 3)\n",
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 2s 799ms/step - loss: 0.4174 - acc: 1.0000 - val_loss: 0.4169 - val_acc: 1.0000\n",
      "Working on a Anomaly video (18/1735)\n",
      "Num frames:  3600\n",
      "data_X shape:  (10, 100, 100, 100, 3)\n",
      "Train on 9 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 1.0707 - acc: 0.0000e+00 - val_loss: 1.0630 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a45e50fedc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5a45e50fedc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_X, data_y)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx1, since it's a grayscalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Training on batches of size 100x100x100x3 by extracting them from video in real-time\n",
    "\n",
    "NAME = \"new-1epochs-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "# Training function\n",
    "def train(data_X, data_y):\n",
    "    # Shuffle data_X and data_y, but make sure each element corresponds to the right label\n",
    "    # Source: https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order\n",
    "    c = list(zip(data_X, data_y))\n",
    "    random.shuffle(c)\n",
    "    data_X, data_y = zip(*c)\n",
    "    \n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx1, since it's a grayscalse\n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx3, since it's not a grayscalse\n",
    "    data_X = np.array(data_X)\n",
    "    data_X = data_X.reshape(-1, NUM_FRAMES_IN_BATCH, FRAME_REZ, FRAME_REZ, 3)\n",
    "    print(\"data_X shape: \", data_X.shape)\n",
    "    # Convert data to be between 0 and 1\n",
    "    #data_X = data_X.astype('float32')\n",
    "    #data_X /= 255\n",
    "    \n",
    "    try:\n",
    "        # Incrementally train the model\n",
    "        # This works according to https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(data_X, data_y, batch_size=1, epochs=1, validation_split=0.1, callbacks=[tensorboard])\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong: \", e)\n",
    "        pass # I know, right?\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "progress = 1\n",
    "\n",
    "# Will train on 1 video at a time\n",
    "# TODO: fix this code from breaking during training on larger videos by training on parts of a video at a time rather than on an entire video\n",
    "for label, vid_path in training_vid_paths:\n",
    "    print(\"Working on a {} video ({}/{})\".format(CATEGORIES[label], progress, len(training_vid_paths)))\n",
    "    progress += 1\n",
    "\n",
    "    # Open a video\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    # Get a number of frames in a video\n",
    "    nframe = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # We will be ignoring leftover frames\n",
    "    nframe -= nframe % NUM_FRAMES_IN_BATCH\n",
    "    print(\"Num frames: \", nframe)\n",
    "    framearray = []\n",
    "    batch_limit = 0\n",
    "\n",
    "    # Only train on bigger video files\n",
    "    if nframe <= NUM_FRAMES_IN_BATCH*4:\n",
    "        continue\n",
    "    \n",
    "    # Only use every 4th frame\n",
    "    for i in range((nframe-NUM_FRAMES_IN_BATCH)//4):\n",
    "        # Skip first batch of frames for better samples\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i*4+NUM_FRAMES_IN_BATCH)\n",
    "        ret, frame = cap.read()\n",
    "        # Resize a frame to proper rezolution\n",
    "        frame = cv2.resize(frame, (FRAME_REZ, FRAME_REZ))\n",
    "        # Convert to grayscale\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Convert to normal RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image = img_to_array(frame)\n",
    "        # reshape to FRAME_REZxFRAME_REZxchannels\n",
    "        image = image.reshape((image.shape[0], image.shape[1], image.shape[2]))\n",
    "        frame = preprocess_input(image)\n",
    "        \n",
    "        # Add to the list\n",
    "        framearray.append(frame)\n",
    "\n",
    "        if len(framearray) == NUM_FRAMES_IN_BATCH:\n",
    "            X.append(framearray)\n",
    "            y.append(label)\n",
    "            batch_limit+=1\n",
    "\n",
    "            # Remove first 3/4ths and shift eveything back\n",
    "            framearray = framearray[int(NUM_FRAMES_IN_BATCH/SKIP_FACTOR):]\n",
    "\n",
    "        # To handle large videos\n",
    "        if batch_limit >= 10:\n",
    "            # Train on 20 bathes of a video at a time\n",
    "            train(X, y)\n",
    "            batch_limit = 0\n",
    "            X = []\n",
    "            y = []\n",
    "            continue\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    train(X, y)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "model.save(\"{}.model\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Image' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a1593fd798e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#plot.imshow(x, cmap=plot.cm.binary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         return _preprocess_symbolic_input(x, data_format=data_format,\n\u001b[0;32m--> 195\u001b[0;31m                                           mode=mode, **kwargs)\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# 'RGB'->'BGR'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m103.939\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m116.779\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123.68\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Image' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# displaying a frame from a broken up batch to make sure it was saved properly\n",
    "\n",
    "# Resahpe data back to the image\n",
    "X = pickle.load(open(\"/home/box/Documents/video/data/Normal/Normal_Videos001_x264.mp4-0.pickle\", \"rb\"))\n",
    "X = X * 255\n",
    "X = X.astype('int32')\n",
    "X = X.reshape(-1, 100, 100)\n",
    "for x in X[:1]:\n",
    "    plot.imshow(x, cmap=plot.cm.binary)\n",
    "    #img = array_to_img(x, data_format='channels_last')\n",
    "    #img = preprocess_input(img)\n",
    "    #print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents127_x264.mp4 video (1/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Robbery/Robbery111_x264.mp4 video (2/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents147_x264.mp4 video (3/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos865_x264.mp4 video (4/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Shoplifting/Shoplifting050_x264.mp4 video (5/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Abuse/Abuse028_x264.mp4 video (6/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Fighting/Fighting026_x264.mp4 video (7/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos404_x264.mp4 video (8/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos226_x264.mp4 video (9/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Vandalism/Vandalism049_x264.mp4 video (10/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Stealing/Stealing046_x264.mp4 video (11/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents151_x264.mp4 video (12/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos357_x264.mp4 video (13/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos567_x264.mp4 video (14/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Fighting/Fighting042_x264.mp4 video (15/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Shooting/Shooting052_x264.mp4 video (16/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Burglary/Burglary025_x264.mp4 video (17/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos615_x264.mp4 video (18/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos491_x264.mp4 video (19/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos756_x264.mp4 video (20/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos392_x264.mp4 video (21/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos763_x264.mp4 video (22/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Vandalism/Vandalism027_x264.mp4 video (23/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos547_x264.mp4 video (24/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos466_x264.mp4 video (25/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Abuse/Abuse021_x264.mp4 video (26/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Robbery/Robbery055_x264.mp4 video (27/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos245_x264.mp4 video (28/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents107_x264.mp4 video (29/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos602_x264.mp4 video (30/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Robbery/Robbery072_x264.mp4 video (31/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Robbery/Robbery032_x264.mp4 video (32/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos833_x264.mp4 video (33/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Explosion/Explosion023_x264.mp4 video (34/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents023_x264.mp4 video (35/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos353_x264.mp4 video (36/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos283_x264.mp4 video (37/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Robbery/Robbery109_x264.mp4 video (38/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Explosion/Explosion027_x264.mp4 video (39/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Burglary/Burglary041_x264.mp4 video (40/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Shooting/Shooting009_x264.mp4 video (41/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Burglary/Burglary033_x264.mp4 video (42/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Shoplifting/Shoplifting008_x264.mp4 video (43/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos477_x264.mp4 video (44/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Abuse/Abuse030_x264.mp4 video (45/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Normal_Videos_event/Normal_Videos_401_x264.mp4 video (46/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Arrest/Arrest014_x264.mp4 video (47/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Arrest/Arrest046_x264.mp4 video (48/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos616_x264.mp4 video (49/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Normal_Videos_event/Normal_Videos_913_x264.mp4 video (50/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Shoplifting/Shoplifting010_x264.mp4 video (51/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Fighting/Fighting018_x264.mp4 video (52/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos382_x264.mp4 video (53/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/RoadAccidents/RoadAccidents141_x264.mp4 video (54/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos748_x264.mp4 video (55/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Vandalism/Vandalism039_x264.mp4 video (56/1735)\n",
      "Working on a /home/box/Downloads/UCF-Anomaly-Detection-Dataset/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos713_x264.mp4 video (57/1735)\n"
     ]
    }
   ],
   "source": [
    "# Break up videos into the fragments of 100x100x100x3 size\n",
    "\n",
    "progress = 1\n",
    "\n",
    "# Will train on 1 video at a time\n",
    "# TODO: fix this code from breaking during training on larger videos by training on parts of a video at a time rather than on an entire video\n",
    "for label, vid_path in training_vid_paths:\n",
    "    print(\"Working on a {} video ({}/{})\".format(vid_path, progress, len(training_vid_paths)))\n",
    "    progress += 1\n",
    "\n",
    "    # Open a video\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    # Get a number of frames in a video\n",
    "    nframe = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # We will be ignoring leftover frames\n",
    "    nframe -= nframe % NUM_FRAMES_IN_BATCH\n",
    "    #print(\"Num frames: \", nframe)\n",
    "    framearray = []\n",
    "    cnt = 0\n",
    "    \n",
    "    # Only train on bigger video files\n",
    "    if nframe <= NUM_FRAMES_IN_BATCH*4:\n",
    "        continue\n",
    "    \n",
    "    # Only use every 4th frame\n",
    "    for i in range((nframe-NUM_FRAMES_IN_BATCH)//4):\n",
    "        # Skip first batch of frames for better samples\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i*4+NUM_FRAMES_IN_BATCH-1)\n",
    "        ret, frame = cap.read()\n",
    "        # Resize a frame to proper rezolution\n",
    "        frame = cv2.resize(frame, (FRAME_REZ, FRAME_REZ))\n",
    "        # Convert to grayscale\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Convert to normal RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image = img_to_array(frame)\n",
    "        # reshape to FRAME_REZxFRAME_REZxchannels\n",
    "        image = image.reshape((image.shape[0], image.shape[1], image.shape[2]))\n",
    "        #frame = preprocess_input(image)\n",
    "        \n",
    "        # Add to the list\n",
    "        framearray.append(frame)\n",
    "\n",
    "        if len(framearray) == NUM_FRAMES_IN_BATCH:\n",
    "            # Save training/testing data\n",
    "            pickle_out = open(\"./data/train/{}-{}.pickle\".format(ntpath.basename(vid_path), cnt), \"wb\")\n",
    "            pickle.dump(framearray, pickle_out)\n",
    "            pickle_out.close()\n",
    "            # Remove first 3/4ths and shift eveything back\n",
    "            framearray = framearray[int(NUM_FRAMES_IN_BATCH/SKIP_FACTOR):]\n",
    "            cnt += 1\n",
    "            \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Loaded shape:  (1, 100, 100, 100, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_5 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0d4597ff4c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     callbacks=[tensorboard])\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}.model\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1928\u001b[0;31m         x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1152\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    333\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have shape (2,) but got array with shape (1,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape:  (1, 100, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# train input 25x100x100x3\n",
    "\n",
    "NAME = \"new-vgg-test-1epochs-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "# Training function\n",
    "def train(data_X, data_y):\n",
    "    # Shuffle data_X and data_y, but make sure each element corresponds to the right label\n",
    "    # Source: https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order\n",
    "    c = list(zip(data_X, data_y))\n",
    "    random.shuffle(c)\n",
    "    data_X, data_y = zip(*c)\n",
    "    \n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx1, since it's a grayscalse\n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx3, since it's not a grayscalse\n",
    "    data_X = np.array(data_X)\n",
    "    data_X = data_X.reshape(-1, NUM_FRAMES_IN_BATCH, FRAME_REZ, FRAME_REZ, 3)\n",
    "    print(\"data_X shape: \", data_X.shape)\n",
    "    # Convert data to be between 0 and 1\n",
    "    #data_X = data_X.astype('float32')\n",
    "    #data_X /= 255\n",
    "    \n",
    "    try:\n",
    "        # Incrementally train the model\n",
    "        # This works according to https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(data_X, data_y, batch_size=1, epochs=1, validation_split=0.1, callbacks=[tensorboard])\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong: \", e)\n",
    "        pass # I know, right?\n",
    "\n",
    "class Vid_Generator(Sequence):\n",
    "    def __init__(self, vid_path, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = []\n",
    "\n",
    "        temp = []\n",
    "\n",
    "        for vid in os.listdir(vid_path):\n",
    "            temp.append(os.path.join(vid_path, vid))\n",
    "\n",
    "        random.shuffle(temp)\n",
    "        self.steps = len(temp) // self.batch_size\n",
    "        self.X = temp\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size))) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_files = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = []\n",
    "\n",
    "        batch_x = []\n",
    "        for x in batch_x_files:\n",
    "            # Read batches of frames\n",
    "            tmp_batch = pickle.load(open(x, \"rb\"))\n",
    "            fin_batch = []\n",
    "            cnt = 0\n",
    "            # For each frame in a batch\n",
    "            for tmp_frame in tmp_batch:\n",
    "                # Convert it to float\n",
    "                tmp_frame = preprocess_input(tmp_frame)\n",
    "                # Add processed frame to new batch\n",
    "                fin_batch.append(tmp_frame)\n",
    "                cnt += 1\n",
    "                \n",
    "                if cnt == 25:\n",
    "                    if \"Normal\" in x:\n",
    "                        batch_y.append(CATEGORIES.index(\"Normal\"))\n",
    "                    else:\n",
    "                        batch_y.append(CATEGORIES.index(\"Anomaly\"))\n",
    "                        \n",
    "                    # Collect all batches together\n",
    "                    batch_x.append(fin_batch)\n",
    "                    fin_batch = []\n",
    "                    cnt = 0\n",
    "            \n",
    "        batch_x = np.array(batch_x)\n",
    "        #print(\"Loaded shape: \", batch_x.shape)\n",
    "        batch_x = batch_x.reshape(-1, batch_x.shape[1], batch_x.shape[2], batch_x.shape[3], batch_x.shape[4])\n",
    "        #print(\"Final shape: \", batch_x.shape)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "training_generator = Vid_Generator(\"./data/train/\", BATCH_SIZE)\n",
    "testing_generator = Vid_Generator(\"./data/test/\", BATCH_SIZE)\n",
    "\n",
    "model.fit_generator(generator=training_generator, \n",
    "                    steps_per_epoch=training_generator.steps, \n",
    "                    epochs=1, verbose=1, \n",
    "                    validation_data=testing_generator, \n",
    "                    validation_steps=testing_generator.steps, \n",
    "                    callbacks=[tensorboard])\n",
    "\n",
    "model.save(\"{}.model\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at a Anomaly video, prediction: [[0.27860102]]\n",
      "Looking at a Anomaly video, prediction: [[0.27860102]]\n",
      "Looking at a Anomaly video, prediction: [[0.27860102]]\n",
      "Looking at a Anomaly video, prediction: [[0.27860102]]\n"
     ]
    }
   ],
   "source": [
    "# test input 25x100x100x3\n",
    "\n",
    "test_model = tf.keras.models.load_model(\"new-vgg-test-1epochs-1553182342.model\")\n",
    "temp = []\n",
    "for vid in os.listdir(\"./data/test/\"):\n",
    "    temp.append(os.path.join(\"./data/test/\", vid))\n",
    "\n",
    "random.shuffle(temp)\n",
    "for x in temp:\n",
    "    # Read batches of frames\n",
    "    tmp_batch = pickle.load(open(x, \"rb\"))\n",
    "    fin_batch = []\n",
    "    cnt = 0\n",
    "    \n",
    "    # For each frame in a batch\n",
    "    for tmp_frame in tmp_batch:\n",
    "        # Convert it to float\n",
    "        tmp_frame = preprocess_input(tmp_frame)\n",
    "        # Add processed frame to new batch\n",
    "        fin_batch.append(tmp_frame)\n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt == 25:\n",
    "            y = []\n",
    "            if \"Normal\" in x:\n",
    "                ind = CATEGORIES.index(\"Normal\")\n",
    "                y = [1, 0]\n",
    "            else:\n",
    "                ind = CATEGORIES.index(\"Anomaly\")\n",
    "                y = [0, 1]\n",
    "            \n",
    "            fin_batch = np.array(fin_batch)\n",
    "            fin_batch = fin_batch.reshape(1, fin_batch.shape[0], fin_batch.shape[1], fin_batch.shape[2], fin_batch.shape[3])\n",
    "\n",
    "            predict = test_model.predict(fin_batch)\n",
    "            print(\"Looking at a {} video, prediction: {}\".format(CATEGORIES[ind], predict))\n",
    "            \n",
    "            # Collect all batches together\n",
    "            fin_batch = []\n",
    "            cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer cu_dnnlstm_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1024]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fe984ad2a213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mencoded_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m                                                          self._num_constants)\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2018.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                            \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                            str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m   1478\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer cu_dnnlstm_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1024]"
     ]
    }
   ],
   "source": [
    "# Model No.2 input 100x100x100x3\n",
    "\n",
    "video = Input(shape=(NUM_FRAMES_IN_BATCH,\n",
    "                     FRAME_REZ, #in the example: channels,\n",
    "                     FRAME_REZ, #rows,\n",
    "                     3))        #in the example: columns))\n",
    "cnn_base = VGG16(input_shape=(FRAME_REZ, #channels,\n",
    "                              FRAME_REZ, #rows,\n",
    "                              3),        #columns),\n",
    "                 weights=\"imagenet\",\n",
    "                 pooling=\"avg\",\n",
    "                 include_top=False)\n",
    "\n",
    "cnn_out = cnn_base.output\n",
    "cnn = Model(cnn_base.input, cnn_out)\n",
    "cnn.trainable = False\n",
    "\n",
    "encoded_frames = TimeDistributed(cnn)(video)\n",
    "encoded_sequence = CuDNNLSTM(256)(encoded_frames)\n",
    "\n",
    "hidden_layer = CuDNNLSTM(256)(Dropout(0.25)(Dense(1024, activation=\"relu\")(encoded_sequence)))\n",
    "hidden_layer = Dropout(0.25)(Dense(1024, activation=\"relu\")(hidden_layer))\n",
    "outputs = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "model = Model([video], outputs)\n",
    "\n",
    "optimizer = Nadam(lr=0.002,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24094\n",
      "2545\n",
      "Epoch 1/10\n",
      "24094/24094 [==============================] - 16617s 690ms/step - loss: 0.6722 - acc: 0.6029 - val_loss: 0.6233 - val_acc: 0.7257\n",
      "Epoch 2/10\n",
      "23543/24094 [============================>.] - ETA: 6:06 - loss: 0.6721 - acc: 0.6031"
     ]
    }
   ],
   "source": [
    "# train input 100x100x100x3\n",
    "\n",
    "NAME = \"100frames-2lstm-vgg-test-10epochs-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "# Training function\n",
    "def train(data_X, data_y):\n",
    "    # Shuffle data_X and data_y, but make sure each element corresponds to the right label\n",
    "    # Source: https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order\n",
    "    c = list(zip(data_X, data_y))\n",
    "    random.shuffle(c)\n",
    "    data_X, data_y = zip(*c)\n",
    "    \n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx1, since it's a grayscalse\n",
    "    # Shape of each image is ANYxNUM_FRAMES_IN_BATCHxFRAME_REZxFRAME_REZx3, since it's not a grayscalse\n",
    "    data_X = np.array(data_X)\n",
    "    data_X = data_X.reshape(-1, NUM_FRAMES_IN_BATCH, FRAME_REZ, FRAME_REZ, 3)\n",
    "    print(\"data_X shape: \", data_X.shape)\n",
    "    # Convert data to be between 0 and 1\n",
    "    #data_X = data_X.astype('float32')\n",
    "    #data_X /= 255\n",
    "    \n",
    "    try:\n",
    "        # Incrementally train the model\n",
    "        # This works according to https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(data_X, data_y, batch_size=1, epochs=1, validation_split=0.1, callbacks=[tensorboard])\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong: \", e)\n",
    "        pass # I know, right?\n",
    "\n",
    "class Vid_Generator(Sequence):\n",
    "    def __init__(self, vid_path, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = []\n",
    "\n",
    "        temp = []\n",
    "\n",
    "        for vid in os.listdir(vid_path):\n",
    "            temp.append(os.path.join(vid_path, vid))\n",
    "\n",
    "        random.shuffle(temp)\n",
    "        self.X = temp\n",
    "        self.steps = len(self.X)\n",
    "        print(self.steps)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_files = self.X[idx:idx + 1]\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for x in batch_x_files:\n",
    "            if \"Normal\" in x:\n",
    "                batch_y.append(CATEGORIES.index(\"Normal\"))\n",
    "            else:\n",
    "                batch_y.append(CATEGORIES.index(\"Anomaly\"))\n",
    "                        \n",
    "            # Read batches of frames\n",
    "            tmp_batch = pickle.load(open(x, \"rb\"))\n",
    "            fin_batch = []\n",
    "\n",
    "            # For each frame in a batch\n",
    "            for tmp_frame in tmp_batch:\n",
    "                # Convert it to float\n",
    "                tmp_frame = preprocess_input(tmp_frame)\n",
    "                # Add processed frame to new batch\n",
    "                fin_batch.append(tmp_frame)\n",
    "                \n",
    "            # Collect all batches together\n",
    "            batch_x.append(fin_batch)\n",
    "            \n",
    "        batch_x = np.array(batch_x)\n",
    "        #print(\"Loaded shape: \", batch_x.shape)\n",
    "        #batch_x = batch_x.reshape(-1, batch_x.shape[1], batch_x.shape[2], batch_x.shape[3], batch_x.shape[4])\n",
    "        #print(\"Final shape: \", batch_x.shape)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "training_generator = Vid_Generator(\"./data/train/\", BATCH_SIZE)\n",
    "testing_generator = Vid_Generator(\"./data/test/\", BATCH_SIZE)\n",
    "\n",
    "model.fit_generator(generator=training_generator, \n",
    "                    steps_per_epoch=training_generator.steps, \n",
    "                    epochs=10, verbose=1, \n",
    "                    validation_data=testing_generator, \n",
    "                    validation_steps=testing_generator.steps, \n",
    "                    callbacks=[tensorboard])\n",
    "\n",
    "model.save(\"{}.model\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
